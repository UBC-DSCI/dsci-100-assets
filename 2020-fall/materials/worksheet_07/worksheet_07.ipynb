{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "517b5b2065555ff098b4bfcfb54f368c",
     "grade": false,
     "grade_id": "cell-83e402ae256bacca",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Worksheet 7 - Classification (Part II)\n",
    "\n",
    "### Lecture and Tutorial Learning Goals:\n",
    "\n",
    "After completing this week's lecture and tutorial work, you will be able to:\n",
    "\n",
    "* Describe what a test data set is and how it is used in classification.\n",
    "* Using R, evaluate classification accuracy using a test data set and appropriate metrics.\n",
    "* Using R, execute cross validation in R to choose the number of neighbours.\n",
    "* Identify when it is necessary to scale variables before classification and do this using R\n",
    "* In a dataset with > 2 attributes, perform k-nearest neighbour classification in R using the `tidymodels` package to predict the class of a test dataset.\n",
    "* Describe advantages and disadvantages of the k-nearest neighbour classification algorithm.\n",
    "\n",
    "This worksheet covers parts of [Chapter 7](https://ubc-dsci.github.io/introduction-to-datascience/classification-continued.html) of the online textbook. You should read this chapter before attempting the worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1efe0801973a1ecc4d5c6f3e199cbf38",
     "grade": false,
     "grade_id": "cell-e3518ed3d64fb67c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Run this cell before continuing.\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "source('tests_worksheet_07.R')\n",
    "source('cleanup_worksheet_07.R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67f52f4e83254e5d82de8b5512fb253c",
     "grade": false,
     "grade_id": "cell-6e0a8546a65b79e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Randomness and Setting Seeds\n",
    "\n",
    "This worksheet uses functions from the `tidymodels` library, which not only allows us to perform K-nearest neighbour classification, but also allows us to evaluate how well our classification worked. In order to ensure that the steps in the worksheet are reproducible, we need to set a *seed*, i.e., a numerical \"starting value,\" which determines the sequence of random numbers R will generate.\n",
    "\n",
    "Below in many cells we have included a call to `set.seed`. **Do not remove these lines of code**; they are necessary to make sure the autotesting code functions properly.\n",
    "\n",
    "> *The reason we have `set.seed` in so many places is that Jupyter notebooks are organized into cells that can be run out of order. Since things can be run out of order, the exact sequence of random values that is used in each cell is hard to determine, which makes autotesting really difficult.  We had two options: either enforce that you only ever run the code by hitting \"Restart & Run All\" to ensure that we get the same values of randomness each time, or put `set.seed` in a lot of places (we chose the latter). One drawback of calling `set.seed` everywhere is that the numbers that will be generated won't really be random. For the purposes of teaching and learning, that is fine here. But __in your course project and other data analyses outside of this course, you must call `set.seed` only once at the beginning of the analysis, so that your random numbers are actually reasonably random.__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc858ec0281c3fc6ae1d90d34a8f7fe1",
     "grade": false,
     "grade_id": "cell-259083aee4faba0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start by loading the `fruit_data.csv` file from the previous tutorial, making sure that we convert the `fruit_name` variable to a \"factor\" data type using `as_factor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50e423755971eb64585d0f05a3631b7c",
     "grade": false,
     "grade_id": "cell-cb697a3b20ed78f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fruit_data <- read_csv('data/fruit_data.csv') %>%\n",
    "                   mutate(fruit_name = as_factor(fruit_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "019d050d7c0e66fc5bb473af7947e51f",
     "grade": false,
     "grade_id": "cell-0e8089ce4f710660",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2. Splitting the data into a training and test set\n",
    "\n",
    "In this exercise, we will be partitioning `fruit_data` into a training (75%) and testing (25%) set using the `tidymodels` package. For this exercise, after creating the test set, we will put the test set away in a lock box and not touch it again until we have found the best k-nn classifier we can make using the training set. We will use the variable `fruit_name` as our class label. \n",
    "\n",
    "\n",
    "**Question 2.0**\n",
    "<br> {points: 1}\n",
    "\n",
    "To create the training and test set, first use the `initial_split` function to split `fruit_data`. Specify you want to use *75%* of the data. For the `strata` argument, place the variable we want to classify, `fruit_name`. Name the object you create `fruit_split`. \n",
    "\n",
    "Next, pass the `fruit_split` object to the `training` and `testing` functions and name your respective objects as `fruit_train` and `fruit_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16bbd34f544a01bf991000b86bbd9b25",
     "grade": false,
     "grade_id": "cell-c262636f2b9777e3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the seed. Don't remove this!\n",
    "set.seed(3456) \n",
    "\n",
    "# Randomly take 75% of the data in the training set. \n",
    "# This will be proportional to the different number of fruit names in the dataset.\n",
    "\n",
    "#... <- initial_split(..., prop = ..., strata = ...)  \n",
    "#... <- training(...)   \n",
    "#... <- testing(...)\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "head(fruit_train)\n",
    "head(fruit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b26c071c7713ad3fe803ab4bd016e677",
     "grade": true,
     "grade_id": "cell-7432078031e563b8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bda459b4cb0afe6372ff3437b1f070a4",
     "grade": false,
     "grade_id": "cell-9f6c24da5042200c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.1** \n",
    "<br> {points: 1}\n",
    "\n",
    "K-nearest neighbors is sensitive to the scale of the predictors so we should some preprocessing to standardize them. Remember that standardizing involves centering/shifting (subtracting the mean of each variable) and scaling (dividing by its standard deviation). Also remember that standardization is *part of your training procedure*, so you can't use your test data to compute the shift / scale values for each variable. Therefore, you must pass only the training data to your recipe to compute the preprocessing steps. This ensures that our test data does not influence any aspect of our model training. Once we have created the standardization preprocessor, we can then later on apply it separately to both the training and test data sets.\n",
    "\n",
    "To scale and center the data, first, pass the vector and the predictors to the `recipe` function. Then, use the `step_scale(all_predictors())` function. This will scale all the predictors. Follow this with the `step_center(all_predictor())` function. This will center all the predictors.  \n",
    "\n",
    "*Assign your answer to an object called `fruit_recipe`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8328112b404d31b57104daa02a0270ce",
     "grade": false,
     "grade_id": "cell-aca4cc578b245211",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#... <- recipe(... ~ ... + ... , data = ...) %>%\n",
    "#    ... %>%\n",
    "#    ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "fruit_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38ce11b9e4c25a61724ed945c13badcc",
     "grade": true,
     "grade_id": "cell-2453aaa31ba794b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6f8793d3f76ccc90ecf4e7ba7afe1c4",
     "grade": false,
     "grade_id": "cell-6693330a0f077083",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question 2.2**\n",
    "<br> {points: 1}\n",
    "\n",
    "So far, we have split the training and testing datasets and preprocessed the data. Now, let's create our K-nearest neighbour classifier with only the training set using the `tidymodels` package. First, create the classifier by specifying that we want $K = 3$ neighbors and that we want to use the *straight-line* distance. \n",
    "\n",
    "*Assign your answer to an object called `knn_spec`*.  \n",
    "\n",
    "Next, train the classifier using the training data set using the `workflow` function. This function allows you to bundle together your pre-processing, modeling, and post-processing requests. Scaffolding is provided below for you.\n",
    "\n",
    "*Assign your answer to an object called `fruit_fit`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de38a024ef006706b0a12a841ebf21d8",
     "grade": false,
     "grade_id": "cell-00b2a6b0ba7df6c4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#... <- nearest_neighbor(weight_func = ..., neighbors = ...) %>%\n",
    "#       set_engine(...) %>%\n",
    "#       set_mode(...)\n",
    "\n",
    "#... <- workflow() %>%\n",
    "#       add_recipe(...) %>%\n",
    "#       add_model(...) %>%\n",
    "#       fit(data = ...)\n",
    "\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "fruit_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7f635299ba4528a5cf607a17c0560af",
     "grade": true,
     "grade_id": "cell-b8ce3699299555d5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c7e3a16d83a86bd933d09d6e5545176",
     "grade": false,
     "grade_id": "cell-512f0c24a1511248",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.3**\n",
    "<br> {points: 1}\n",
    "\n",
    "Now that we have created our K-nearest neighbor classifier object, let's predict the class labels for our test set.\n",
    "\n",
    "First, pass your fitted model and the testing dataset to the `predict` function. Then, use the `bind_cols` function to add the column of predictions to the original test data. \n",
    "\n",
    "*Assign your answer to an object called `fruit_test_predictions`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a769709bff0f2a4076011456652fc978",
     "grade": false,
     "grade_id": "cell-29a65e0d69fd64d3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#... <- predict(... , ...) %>%\n",
    "#       ...(...)\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "fruit_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f76e36541eb840d9aa116186c3d19d12",
     "grade": true,
     "grade_id": "cell-144f33352d2b6add",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "928accd43fcde1e014140a1f218113cc",
     "grade": false,
     "grade_id": "cell-baa81bf7577a344f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question 2.4**\n",
    "<br> {points: 1}\n",
    "\n",
    "Great! We have now computed some predictions for our test datasets! Wouldn't it be interesting if we could find out our classifier's accuracy? \n",
    "\n",
    "Thankfully, the `metrics` function from the `tidymodels` package can help us. To get the statistics about the quality of our model, you need to specify the `truth` and `estimate` arguments. In the `truth` argument, you should put the column name for the true values of the response variable. In the `estimate` argument, you should put the column name for response variable predictions. \n",
    "\n",
    "*Assign your answer to an object called `fruit_prediction_accuracy`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53639b8dded722b174dcb3045484928e",
     "grade": false,
     "grade_id": "cell-4ac19dfcaec7e27c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#... <- fruit_test_predictions %>%\n",
    "#         ...(truth = ..., estimate = ...)             \n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "fruit_prediction_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6518c3556f30d94035a357daab620cb6",
     "grade": true,
     "grade_id": "cell-a268da3370127d6c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3480a1a7545da589eb5f19181e4f63c0",
     "grade": false,
     "grade_id": "cell-67a957c51db2a297",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question 2.5**\n",
    "<br> {points: 1}\n",
    "\n",
    "It appears that the accuracy of the classifier was **77%**. Let's also look at the *confusion matrix* for the classifier. This will show us the table of predicted labels and correct labels. \n",
    "\n",
    "A confusion matrix is essentially a classification matrix. The columns of the confusion matrix represent the actual class and the rows represent the predicted class (or vice versa).\n",
    "\n",
    "|                  |          |  Actual Values |                |\n",
    "|:----------------:|----------|:--------------:|:--------------:|\n",
    "|                  |          |    Positive    |    Negative    |\n",
    "|**Predicted Value**  | Positive |  True Positive | False Positive|\n",
    "|                  | Negative | False Negative | True Negative  |\n",
    "\n",
    "\n",
    "- A **true positive** is an outcome where the model correctly predicts the positive class.\n",
    "- A **true negative** is an outcome where the model correctly predicts the negative class.\n",
    "- A **false positive** is an outcome where the model incorrectly predicts the positive class.\n",
    "- A **false negative** is an outcome where the model incorrectly predicts the negative class.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can create a confusion matrix by using the `conf_mat` function. Similar to the `metrics` function, you will have to specify the `truth` and `estimate` arguments. \n",
    "\n",
    "*Assign your answer to an object called `fruit_mat`*.\n",
    "\n",
    "*Note: 77% might be good enough or not good enough, depending on the application!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9920c7b660f14fa77265065676c77e89",
     "grade": false,
     "grade_id": "cell-f0716222fa3e3c93",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#... <- fruit_test_predictions %>% \n",
    "#       ...(truth = ..., estimate = ...)\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "fruit_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50ba35a8c51cc1a58b6bdbbb2dcfbda0",
     "grade": true,
     "grade_id": "cell-d3d5c46250e8450a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8c0fd4a2a74379daa5d281b490863d9",
     "grade": false,
     "grade_id": "cell-76907faaa508f23b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.6** Multiple Choice:\n",
    "<br> {points: 1}\n",
    "\n",
    "Reading `fruit_mat`, how many observations were labelled correctly?\n",
    "\n",
    "A. 7\n",
    "\n",
    "B. 8\n",
    "\n",
    "C. 9\n",
    "\n",
    "D. 10\n",
    "\n",
    "*Assign your answer to an object called `answer2.6`. Make sure your answer is an uppercase letter and is surrounded by quotation marks (e.g. `\"F\"`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7dc42422b7bc986c7a575478f241a04",
     "grade": false,
     "grade_id": "cell-c4d8f09cbf40df59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "answer2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a8cb4af39a241cd0760105114251180",
     "grade": true,
     "grade_id": "cell-5715f106e3bbf143",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e125433c1b137f25301bf859c863a30",
     "grade": false,
     "grade_id": "cell-0063191266f982c7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 3. Cross-validation\n",
    "\n",
    "**Question 3.1**\n",
    "<br> {points: 1}\n",
    "\n",
    "The vast majority of predictive models in statistics and machine learning have parameters that you have to pick. For the past few exercises, we have had to pick the number of neighbours for the class vote. But, is it possible to make this selection, *i.e., tune the model, in a principled way?* Ideally, we want to maximize the performance of our classifier on data *it hasn’t seen yet*.\n",
    "\n",
    "There is also an important detail to mention about the process of tuning: we can, if we want to, split our overall training data up in multiple different ways, train and evaluate a classifier for each split, and then choose the parameter based on all of the different results. If we just split our overall training data once, our best parameter choice will depend strongly on whatever data was lucky enough to end up in the validation set. Perhaps using multiple different train / validation splits, we’ll get a better estimate of accuracy, which will lead to a better choice of the number of neighbours $K$ for the overall set of training data. \n",
    "\n",
    "This leads to the idea of cross-validation. In cross-validation, we split our overall training data into $C$ evenly-sized chunks, and then iteratively use 1 chunk as the validation set and combine the remaining $C−1$ chunks as the **training set.**\n",
    "\n",
    "We can perform a cross-validation in R using the `vfold_cv` function. To use this function, you have to identify the training set as well as specify the `v` (the number of folds) and `strata` (the label variable) arguments.   \n",
    "\n",
    "*Assign your answer to an object called `fruit_vfold`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed6f4004f969f9484c20a1f1a926dd27",
     "grade": false,
     "grade_id": "cell-40206dc81c114f74",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#... <- vfold_cv(..., v = ..., strata = ...)\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b591b806779aacafd2bb2a3e4511ae01",
     "grade": true,
     "grade_id": "cell-4f7b62f0f7b0a200",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_3.1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d1e589450a8e1099c29da82263525f7",
     "grade": false,
     "grade_id": "cell-068da1c1156e503a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question 3.2**\n",
    "<br> {points: 1}\n",
    "\n",
    "Now perform the workflow analysis again. You can reuse the `fruit_recipe` and `knn_spec` objects you made earlier. When you are fitting the knn model, use the `fit_resamples` function instead of the `fit` function for training. This function will allow us to run a cross-validation on each train/validation split we created in the previous question. \n",
    "\n",
    "*Assign your answer to an object called `fruit_resample_fit`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9650c6e33bdec2825a3372e62829c2d6",
     "grade": false,
     "grade_id": "cell-0474eb947592495c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#... <- workflow() %>%\n",
    "#       add_recipe(...) %>%\n",
    "#       add_model(...) %>%\n",
    "#       fit_resamples(resamples = ...)\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f8e388622fdde399326dcbb41f693bd",
     "grade": true,
     "grade_id": "cell-099fb002dba6ced0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_3.2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40bdb9b98b1c06b579eb2d8f5e162461",
     "grade": false,
     "grade_id": "cell-df3dc35f0cbb35e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3.3** \n",
    "<br> {points: 1}\n",
    "\n",
    "Now that we have ran a cross-validation on each train/validation split, one has to ask, how accurate was the classifier's validation across the folds? We can aggregate the *mean* and *standard error* by using the `collect_metrics` function. The standard error is essentially a measure of how uncertain we are in the mean value. \n",
    "\n",
    "Use the `collect_metrics` function on the `fruit_resample_fit` object and assign your answer to an object called `fruit_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa2b50e63553e0fab6b486d57fad0794",
     "grade": false,
     "grade_id": "cell-1119165a08beb72d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "fruit_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff6f588520207e25a68b527978186260",
     "grade": true,
     "grade_id": "cell-7337cd2a0af4b896",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_3.3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a15da214f12c202edef4753d61a1dd0",
     "grade": false,
     "grade_id": "cell-b65f60bbbc0a4884",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 4. Parameter value selection\n",
    "\n",
    "Using a 5-fold cross-validation, it is estimated that the prediction accuracy of our classifier is somewhere around 94%. Again, this may be great, OK, or bad, depending on the application. \n",
    "\n",
    "If we had to improve our classifier, we have to change the parameter: number of neighbours, $K$. Since cross-validation helps us evaluate the accuracy of our classifier, we can use cross-validation to calculate an accuracy for each value of $K$ in a reasonable range, and then pick the value of $K$ that gives us the best accuracy. \n",
    "\n",
    "The great thing about the `tidymodels` package is that it provides a very simple syntax for tuning models. Using `tune()`, each parameter in the model can be adjusted rather than given a specific value. \n",
    "\n",
    "**Question 4.0**\n",
    "<br> {points: 1}\n",
    "\n",
    "Create a new K-nearest neighbor model specification but instead of specifying a particular value for the `neighbors` argument, insert `tune()`. \n",
    "\n",
    "*Assign your answer to an object called `knn_tune`.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b6ac3556842da4c5c7c9b9c314bab46",
     "grade": false,
     "grade_id": "cell-2d0448675438bdec",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "knn_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccc6b19b3179ffd5a1ba1b251fcfb151",
     "grade": true,
     "grade_id": "cell-381d21c4c1474e80",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_4.0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa2d28b03b7815824429b0dd0d7c7a3d",
     "grade": false,
     "grade_id": "cell-706cde9c0d8d2699",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 4.1**\n",
    "<br>{points: 1}\n",
    "\n",
    "Now, create a `workflow()` analysis that combines `fruit_recipe` and our new `knn_tune` model specification. \n",
    "\n",
    "Instead of using `fit` or `fit_resamples`, we will use the `tune_grid` function to fit the model for each value in a range of parameter values. For the `resamples` argument, input the cross-validation `fruit_vfold` model we created earlier. The `grid` argument specifies that the tuning should try $X$ amount of values of the number of neighbors $K$ when tuning. For this exercise, use *10 $K$ values when tuning*. \n",
    "\n",
    "Finally, aggregate the mean and standard error by using the `collect_metrics` function.\n",
    "\n",
    "*Assign your answer to an object called `knn_results`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edf19a3768485fa5c2e5235cb4685d10",
     "grade": false,
     "grade_id": "cell-c0d2f1edbe342214",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "set.seed(1234) # set the seed, don't remove this\n",
    "\n",
    "#... <- workflow() %>%\n",
    "#       add_recipe(...) %>%\n",
    "#       add_model(...) %>%\n",
    "#       tune_grid(resamples = ..., grid = )\n",
    "#       ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2d9381f3f1c945a4f208684ea04e6b5",
     "grade": true,
     "grade_id": "cell-0cbc6a47a2b14884",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_4.1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a60ce3465536d0a1c1c38957b89569c",
     "grade": false,
     "grade_id": "cell-e8a71fe7a9789225",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 4.2**\n",
    "<br>{points: 1} \n",
    "\n",
    "\n",
    "Now, let's find the best value of the number of neighbors. \n",
    "\n",
    "First, from `knn_results`, filter for `accuracy` from the `.metric` column. \n",
    "\n",
    "*Assign your answer to an object called `accuracies`.*\n",
    "\n",
    "Next, create a line plot using the `accuracies` dataset with `neighbors` on the x-axis and the `mean` on the y-axis. \n",
    "\n",
    "*Assign your answer to an object called `accuracy_versus_k`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79027df9a634c82d8864326404628158",
     "grade": false,
     "grade_id": "cell-b8ae5cb978d3f8f2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#... <- knn_results %>% \n",
    "#       filter(...)\n",
    "\n",
    "#... <- ggplot(..., aes(x = ..., y = ...))+\n",
    "#       geom_point() +\n",
    "#       geom_line() +\n",
    "#       labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "#       scale_x_continuous(breaks = seq(0, 14, by = 2)) +  # adjusting the x-axis\n",
    "#       scale_y_continuous(limits = c(0.6, 1.0)) # adjusting the y-axis\n",
    "\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "accuracy_versus_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d36c5097a082447e87d5b791da2a3696",
     "grade": true,
     "grade_id": "cell-6d62d960eaff34d6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_4.2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f67015a477676ab3299857bfa926c6f3",
     "grade": false,
     "grade_id": "cell-ff9e1c1f4f70c1e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "From the plot above, we can see that $K = 3$ or $4$ provides the highest accuracy. Larger $K$ values result in a reduced accuracy estimate. Remember: the values you see on this plot are estimates of the true accuracy of our classifier. Although the $K = 3$ or $4$ value is higher than the others on this plot, that doesn’t mean the classifier is necessarily more accurate with this parameter value! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b96cf43f98ea9bf9e2c9958a497023ff",
     "grade": false,
     "grade_id": "cell-0b119208147a50fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Great, now you have completed a full workflow analysis with cross-validation using the `tidymodels` package! For your information, we can choose any number of folds and typically, the more we use the better our accuracy estimate will be (lower standard error). However, more folds would mean a greater computation time. In practice, $C$ is chosen to be either 5 or 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5765aea12e32391eb01142a32e40d290",
     "grade": false,
     "grade_id": "cell-4609bfd4722e6480",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "source('cleanup_worksheet_07.R')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
